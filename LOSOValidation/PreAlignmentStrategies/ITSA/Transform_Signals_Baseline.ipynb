{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e7debf-bbfd-49a5-89c3-211af6ca1203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.classification import TSclassifier\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.mean import mean_logeuclid\n",
    "#from pyriemann.classification import SVC\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import h5py\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53e10c7-1b31-4980-9ebf-da1828078ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f5306ce-9a7b-4bd0-92d5-a28f25724bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LoadingPath_Non='.../Ns_100_Individual/Advance'\n",
    "\n",
    "SavingPath = '.../Ns_100_Individual/Advance/Transformed/Ind_TS_Transformed/Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f0cf41-a089-4cb6-925d-7b9d9c38f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af047c8-de32-4e97-9afc-62a9c8149cff",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56e279f-f83d-44b6-95c0-1ffebee10345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def load_testing_data(testing_loading_path, test_subject_id, n_components=108):\n",
    "    \"\"\"\n",
    "    Load (raw)training and testing datasets for a given subject.\n",
    "    \n",
    "    Parameters:\n",
    "        loading_path (str): Path to the directory containing the dataset files.\n",
    "        subject_id (int): Subject ID (0-based index).\n",
    "        n_components (int): Number of components/features to extract.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing all loaded datasets and features.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    testing_dataset_path = os.path.join(testing_loading_path, 'TestingDataset.mat') \n",
    " \n",
    "    with h5py.File(testing_dataset_path, 'r') as f_testing_dataset:\n",
    "            testing_dataset = f_testing_dataset['TestingDataset']\n",
    "            subject_cell_ref = testing_dataset[test_subject_id, 0]\n",
    "            subject_data = f_testing_dataset[subject_cell_ref]\n",
    "            X_test_dataset = np.transpose(subject_data['x'][:], (0, 1, 2))\n",
    "            Y_test_dataset = np.squeeze(subject_data['y'][:])\n",
    "\n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    data = {\n",
    "        \"X_test_dataset\": X_test_dataset,\n",
    "        \"Y_test_dataset\": Y_test_dataset,\n",
    "    }\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37e3e26-115d-4e33-b3f8-6d60baf8e225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def load_training_subject_data(training_loading_path, test_subject_id, train_subject_id, n_components=108):\n",
    "    \"\"\"\n",
    "    Load (raw)training and testing datasets for a given subject.\n",
    "    \n",
    "    Parameters:\n",
    "        loading_path (str): Path to the directory containing the dataset files.\n",
    "        subject_id (int): Subject ID (0-based index).\n",
    "        n_components (int): Number of components/features to extract.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing all loaded datasets and features.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    training_dataset_path = os.path.join(training_loading_path, 'TrainingDataset.mat') \n",
    " \n",
    "    with h5py.File(training_dataset_path, 'r') as f_training_dataset:\n",
    "        training_dataset = f_training_dataset['TrainingDataset']\n",
    "        training_cell_ref = training_dataset[test_subject_id,0]\n",
    "        training_cell = f_training_dataset[training_cell_ref]\n",
    "\n",
    "        # Access the specific train subject inside the nested 1x17 cell\n",
    "        subject_ref = training_cell[train_subject_id,0]\n",
    "        subject_data = f_training_dataset[subject_ref]\n",
    "\n",
    "        X_train_dataset = np.transpose(subject_data['x'][:], (0, 1, 2))\n",
    "        Y_train_dataset = np.squeeze(subject_data['y'][:])\n",
    " \n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    data = {\n",
    "        \"X_train_dataset\": X_train_dataset,\n",
    "        \"Y_train_dataset\": Y_train_dataset,\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe947fa-ef16-4608-bdc3-ef65a8b6b775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recentering function \n",
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from pyriemann.utils.tangentspace import tangent_space\n",
    "\n",
    "def Recentering_TSProjection(C, M):\n",
    "    \"\"\"\n",
    "    1. Centers each covariance matrix: M^(-1/2) C M^(-1/2))\n",
    "    2. Projects to TS (& vectorises) using identity matrix as 'new centre'\n",
    "    \n",
    "    Parameters:\n",
    "        C (ndarray): set of covariance matrices (Nt, Nc, Nc) \n",
    "        M (ndarray): Reference matrix (Taken as log-eulid mean)\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: The log-transformed matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    Nt, _, _ = C.shape # Number of trials \n",
    "    \n",
    "    M_inv_sqrt = fractional_matrix_power(M, -0.5) # Compute M^(-1/2)\n",
    "    \n",
    "    # 1. Recentering step (on each covariance matrix) \n",
    "    X_centered = np.array([M_inv_sqrt @ C[i] @ M_inv_sqrt for i in range(Nt)])\n",
    "    #print(X_centered.shape)\n",
    "    \n",
    "    #2. TS projection \n",
    "    Centered_M = np.eye(C.shape[1]) # New center after centering is the identity matrix \n",
    "    X_TS = tangent_space(X_centered, Centered_M) # Project to TS (& vectorise)\n",
    "\n",
    "    \n",
    "    return X_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ecc920-a2d3-41eb-99da-5c9974d79fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Rescaling(C):\n",
    "    \"\"\"\n",
    "    Rescale all tangent vectors: c_tilde = c / (1/N_t * sum_n ||c_n||)\n",
    "\n",
    "    Parameters:\n",
    "        C (ndarray): Collection of tangent vectors (c_n) to be rescaled\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Rescaled tangents vectors `c_tilde`.\n",
    "    \"\"\"\n",
    "    \n",
    "    Nt = C.shape[0] # Number of vectors\n",
    "    \n",
    "    # Check if C contains any 'nan' and replace with 0.0\n",
    "    if np.isnan(C).any():\n",
    "        C = np.nan_to_num(C, nan=0.0)\n",
    "    \n",
    "    # Compute the average magnitude (1/N_t * sum(||c_n||))\n",
    "    avg_magnitude = np.sum(np.linalg.norm(C, axis=1)) / Nt\n",
    "\n",
    "    # Normalize all vectors simultaneously\n",
    "    c_tilde  = C / avg_magnitude\n",
    "\n",
    "    return c_tilde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa67384-2202-45f6-8706-e155c6ec802d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rotation Functions\n",
    "\n",
    "\n",
    "# Mean per class \n",
    "def ClassMean(Dataset, Labels):\n",
    "    \"\"\"\n",
    "    Compute the class-wise mean based on the provided equation:\n",
    "        s̄_k = (1 / N_k) * Σ s̃_i  (for y_i = k)\n",
    "    \n",
    "    Parameters:\n",
    "        Dataset (ndarray): Tangent vectors, Nt = number of trials, Nf = features per trial.\n",
    "        Labels (ndarray): The corresponding labels for the dataset, of shape (Nt,).\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: matrix of shape (k, Nf) (k = number of classes)\n",
    "    \"\"\"\n",
    "    unique_classes = np.unique(Labels)  # Find unique classes \n",
    "    class_means = []\n",
    "\n",
    "    for cls in unique_classes: # for each class\n",
    "        # Select data corresponding to the current class\n",
    "        class_data = Dataset[Labels == cls]  # Filter rows where label == cls\n",
    "        N_k = class_data.shape[0]  # Number of trials for class k\n",
    "        \n",
    "        # Compute mean\n",
    "        class_mean = np.mean(class_data, axis=0)\n",
    "        \n",
    "        # Store result\n",
    "        class_means.append(class_mean)\n",
    "    \n",
    "    C_bar = np.column_stack(class_means)\n",
    "\n",
    "    return C_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025eb33c-6b6d-4282-b566-4de485ebf87a",
   "metadata": {},
   "source": [
    "# Load subject's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf5d8227-f1ee-40c9-aa8c-9e5924a0b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b589a216-687e-47a6-bbc7-94d41c5be734",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pickle \n",
    "New_dataset = \"Yes\"\n",
    "\n",
    "if New_dataset == \"No\":\n",
    "    TransformedTraining_path = os.path.join(SavingPath, 'TransformedTraining.pkl')\n",
    "    TransformedTesting_path = os.path.join(SavingPath, 'TransformedTesting.pkl')\n",
    "\n",
    "    TransformedTraining = load_data(TransformedTraining_path)\n",
    "    TransformedTesting = load_data(TransformedTesting_path)\n",
    "\n",
    "elif New_dataset == \"Yes\":\n",
    "    TransformedTraining = {}\n",
    "    TransformedTesting = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f8ca1b7-258e-4aaf-b795-b6800c5b9819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing subject 0:\n",
      "Loaded testing datatsets shape: \n",
      "(2777, 108, 100)\n",
      "Labels sizes: Training (43450,), Testing (2777,)\n",
      "Centred shapes: \n",
      "(43450, 5886)\n",
      "(2777, 5886)\n",
      "Rescaled shapes: \n",
      "(43450, 5886)\n",
      "(2777, 5886)\n",
      "Fold index: 0\n",
      "Indices for calibration subset: [   1    2    6 ... 2771 2773 2776]\n",
      "Indices for testing subset: [   0    3    4 ... 2772 2774 2775]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(1388, 5886)\n",
      "Rotated Test: \n",
      "(1389, 5886)\n",
      "Fold index: 1\n",
      "Indices for calibration subset: [   0    3    4 ... 2772 2774 2775]\n",
      "Indices for testing subset: [   1    2    6 ... 2771 2773 2776]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(1389, 5886)\n",
      "Rotated Test: \n",
      "(1388, 5886)\n",
      "Testing subject 1:\n",
      "Loaded testing datatsets shape: \n",
      "(2808, 108, 100)\n",
      "Labels sizes: Training (43419,), Testing (2808,)\n",
      "Centred shapes: \n",
      "(43419, 5886)\n",
      "(2808, 5886)\n",
      "Rescaled shapes: \n",
      "(43419, 5886)\n",
      "(2808, 5886)\n",
      "Fold index: 0\n",
      "Indices for calibration subset: [   2    4    7 ... 2803 2806 2807]\n",
      "Indices for testing subset: [   0    1    3 ... 2798 2804 2805]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(1404, 5886)\n",
      "Rotated Test: \n",
      "(1404, 5886)\n",
      "Fold index: 1\n",
      "Indices for calibration subset: [   0    1    3 ... 2798 2804 2805]\n",
      "Indices for testing subset: [   2    4    7 ... 2803 2806 2807]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(1404, 5886)\n",
      "Rotated Test: \n",
      "(1404, 5886)\n",
      "Testing subject 2:\n",
      "Loaded testing datatsets shape: \n",
      "(2495, 108, 100)\n",
      "Labels sizes: Training (43732,), Testing (2495,)\n",
      "Centred shapes: \n",
      "(43732, 5886)\n",
      "(2495, 5886)\n",
      "Rescaled shapes: \n",
      "(43732, 5886)\n",
      "(2495, 5886)\n",
      "Fold index: 0\n",
      "Indices for calibration subset: [   0    6    7 ... 2490 2491 2494]\n",
      "Indices for testing subset: [   1    2    3 ... 2486 2492 2493]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(1247, 5886)\n",
      "Rotated Test: \n",
      "(1248, 5886)\n",
      "Fold index: 1\n",
      "Indices for calibration subset: [   1    2    3 ... 2486 2492 2493]\n",
      "Indices for testing subset: [   0    6    7 ... 2490 2491 2494]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(1248, 5886)\n",
      "Rotated Test: \n",
      "(1247, 5886)\n"
     ]
    }
   ],
   "source": [
    "TransformedTraining = {}\n",
    "TransformedTesting = {}\n",
    "indices_split = {}\n",
    "\n",
    "subject_range = 'sub001_003'\n",
    "Fold_Ids = [0, 1, 2]\n",
    "#Fold_Ids = [0, 1, 2, 3, 4]\n",
    "#Fold_Ids = [5, 6, 7, 8, 9]\n",
    "N_train = 17\n",
    "N_folds = 3\n",
    "\n",
    "#for test_sub_id in [0]:\n",
    "for test_sub_id in Fold_Ids:\n",
    "    print(f\"Testing subject {test_sub_id}:\")\n",
    "    test_subject_data = load_testing_data(LoadingPath_Non, test_sub_id, n_components=Nc)\n",
    "\n",
    "\n",
    "    X_test_dataset = test_subject_data[\"X_test_dataset\"]\n",
    "    Y_test_dataset = test_subject_data[\"Y_test_dataset\"]\n",
    "    \n",
    "    \n",
    "    # Sanity check\n",
    "    print(\"Loaded testing datatsets shape: \")\n",
    "    print(X_test_dataset.shape)\n",
    "    \n",
    "# ---------------------- 1. Recentre (TESTING) - recentres all trials to have centre of mass equal to identity matrix, followed by tangent space mapping (ref = I) ----------------------     \n",
    "    # Find covariance matrices\n",
    "    cov_estimator = Covariances(estimator='lwf')\n",
    "    Test_cov = cov_estimator.transform(X_test_dataset)\n",
    "\n",
    "    # Find log-euclid centre ('M')\n",
    "    Test_M = mean_logeuclid(Test_cov)\n",
    "\n",
    "    # Centre and project to TS (& vectorise)\n",
    "    Test_Centered = Recentering_TSProjection(Test_cov, Test_M)\n",
    "\n",
    "\n",
    "    # ---------------------- 1. Recentre (TRAINING) - recentres all trials to have centre of mass equal to identity matrix, followed by tangent space mapping (ref = I) ----------------------     \n",
    "    Train_Centered_list = []\n",
    "    Y_train_list=[]\n",
    "    \n",
    "    # Load each (ind) tranining dataset\n",
    "    for train_sub_id in range(N_train):\n",
    "        #print(f\"Training subject {train_sub_id}:\")\n",
    "        train_subject_data = load_training_subject_data(LoadingPath_Non, test_sub_id, train_sub_id, n_components=Nc)\n",
    "\n",
    "\n",
    "        X_train_dataset = train_subject_data[\"X_train_dataset\"]\n",
    "        Y_train_sub_dataset = train_subject_data[\"Y_train_dataset\"]\n",
    "    \n",
    "\n",
    "        # Sanity check\n",
    "        # print(\"Loaded training datatsets shape: \")\n",
    "        # print(X_train_dataset.shape)\n",
    "        \n",
    "        # Find covariance matrices\n",
    "        cov_estimator = Covariances(estimator='lwf')\n",
    "        Train_cov = cov_estimator.transform(X_train_dataset)\n",
    "\n",
    "        # Find log-euclid centre ('M')\n",
    "        Train_M = mean_logeuclid(Train_cov)\n",
    "\n",
    "        # Centre and project to TS (& vectorise)\n",
    "        Train_sub_Centered = Recentering_TSProjection(Train_cov, Train_M)\n",
    "        \n",
    "        # Concatenate each subjects' centred vector and corresponding labels \n",
    "        Train_Centered_list.append(Train_sub_Centered)  \n",
    "        Y_train_list.append(Y_train_sub_dataset)\n",
    "        \n",
    "    Train_Centered = np.concatenate(Train_Centered_list, axis=0)\n",
    "    Y_train_dataset = np.concatenate(Y_train_list, axis = 0)\n",
    "    \n",
    "    print(f\"Labels sizes: Training {Y_train_dataset.shape}, Testing {Y_test_dataset.shape}\")\n",
    "    \n",
    "    #Sanity Check\n",
    "    print(\"Centred shapes: \")\n",
    "    print(Train_Centered.shape)\n",
    "    print(Test_Centered.shape)\n",
    "    \n",
    "    # ---------------------- 2. Rescale - match matrix dispersion around mean in both 'source' and 'target' (setting the average norm within set to be 1) ----------------------\n",
    "    Train_Rescale = Rescaling(Train_Centered)\n",
    "    Test_Rescale = Rescaling(Test_Centered)\n",
    "\n",
    "    #Sanity Check\n",
    "    print(\"Rescaled shapes: \")\n",
    "    print(Train_Rescale.shape)\n",
    "    print(Test_Rescale.shape)\n",
    "    \n",
    "    # ---------------------- 3. Rotation (Alignment of 'target' vectors) - Align each mean of each class as much as possible (using Eulidea Procrustes procedure)  ----------------------\n",
    "    # ---------------------- Once target vectors are aligned, can be used with models trained using 'Train_Rescale' ---------------------- \n",
    "    # Calculate anchor points for each class \n",
    "    \n",
    "    \n",
    "    # Split Testing into calibration and testing \n",
    "    Test_Rotated_list=[]\n",
    "    Test_labels_list=[]\n",
    "    \n",
    "    # Store indices split per testing subject and fold for consistency across Baseline and SpatFilt\n",
    "    calibration_idx_list = []\n",
    "    test_idx_list = []\n",
    "    \n",
    "    indices_split[test_sub_id] = {}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42) # 50:50 split calibration and testing\n",
    "    for fold_idx, (calibration_idx, test_idx) in enumerate(skf.split(Test_Rescale, Y_test_dataset)):\n",
    "    \n",
    "        print(f\"Fold index: {fold_idx}\")\n",
    "        print(f\"Indices for calibration subset: {calibration_idx}\")\n",
    "        print(f\"Indices for testing subset: {test_idx}\")\n",
    "        \n",
    "        # Store per testing subject and fold\n",
    "        indices_split[test_sub_id][f\"Fold{fold_idx + 1}\"] = {\n",
    "            \"calibration_idx\": calibration_idx,\n",
    "            \"test_idx\": test_idx\n",
    "        }\n",
    "\n",
    "\n",
    "        X_calibration, X_test = Test_Rescale[calibration_idx], Test_Rescale[test_idx]\n",
    "        Y_calibration, Y_test = Y_test_dataset[calibration_idx], Y_test_dataset[test_idx]\n",
    "        \n",
    "\n",
    "\n",
    "        Train_AnchorPoints=ClassMean(Train_Rescale, Y_train_dataset)\n",
    "        Test_AnchorPoints=ClassMean(X_calibration, Y_calibration) # Only calibration subset\n",
    "\n",
    "        # Sanity check\n",
    "        print(\"Anchor Points shape: \")\n",
    "        print(Train_AnchorPoints.shape)\n",
    "        print(Test_AnchorPoints.shape)\n",
    "\n",
    "        # Cross-product matrix \n",
    "        c_st = Train_AnchorPoints @ Test_AnchorPoints.T\n",
    "\n",
    "        #Sanity check - should be (Nf, Nf)\n",
    "        print(\"c_st shape: \")\n",
    "        print(c_st.shape) \n",
    "\n",
    "        # Perform Singular value decomposition on c_st\n",
    "        U, D, VT = np.linalg.svd(c_st, full_matrices=False)\n",
    "\n",
    "        # Find number of Nv vectors that explains 99.9% varaince \n",
    "        explained_variance = D**2\n",
    "        total_variance = np.sum(explained_variance)\n",
    "        cumulative_explained_variance = np.cumsum(explained_variance) / total_variance\n",
    "        Nv = np.argmax(cumulative_explained_variance >= 0.999) + 1  # +1 because of 0-based indexing\n",
    "\n",
    "        U_tilde = U[:, :Nv] # Truncate using only Nv vectors\n",
    "        VT_tilde = VT[:Nv, :]\n",
    "\n",
    "        # Sanity check \n",
    "        print(\"truncated U and VT: \")\n",
    "        print(U_tilde.shape)\n",
    "        print(VT_tilde.shape)\n",
    "\n",
    "        print(X_calibration.shape)\n",
    "\n",
    "        Nt = X_test.shape[0]\n",
    "        Test_Rotated = np.zeros_like(X_test) # Initialise storage\n",
    "\n",
    "        # Align each testing trial\n",
    "        for t in range(Nt):\n",
    "            Test_Rotated[t] = U_tilde @ VT_tilde @ X_test[t]\n",
    "\n",
    "        print(\"Rotated Test: \")\n",
    "        print(Test_Rotated.shape)\n",
    "\n",
    "         # Save for this fold\n",
    "        Test_Rotated_list.append(Test_Rotated)\n",
    "        Test_labels_list.append(Y_test)\n",
    "\n",
    "        if test_sub_id not in TransformedTesting:\n",
    "            TransformedTesting[test_sub_id] = {}\n",
    "\n",
    "        TransformedTesting[test_sub_id][f\"Fold{fold_idx + 1}\"] = {\n",
    "            \"x\": Test_Rotated,\n",
    "            \"y\": Y_test\n",
    "        }\n",
    "\n",
    "    if test_sub_id not in TransformedTraining:\n",
    "        TransformedTraining[test_sub_id] = {}\n",
    "        \n",
    "    # Store per subject (transformed training and testing datasets)\n",
    "    TransformedTraining[test_sub_id] = {\n",
    "        \"x\": Train_Rescale,\n",
    "        \"y\": Y_train_dataset\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "263b96b8-f07c-4c03-82d7-5139d84da580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 20.86174545288086 minutes\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc73d98-895b-4480-9232-bdb361fa599f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.../Ns_100_Individual/Advance/Transformed/Ind_TS_Transformed/Baseline'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SavingPath"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c90965c-0739-4a03-b894-24959e8c612a",
   "metadata": {},
   "source": [
    "print(SavingPath)\n",
    "print(os.path.exists(SavingPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5347b1b6-ccdd-4dc3-8b93-540ccecaeb82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define saving paths for the variables\n",
    "training_saving_path = os.path.join(SavingPath, f'TransformedTraining_{subject_range}.pkl')\n",
    "testing_saving_path = os.path.join(SavingPath, f'TransformedTesting_{subject_range}.pkl')\n",
    "\n",
    "indices_saving_path = os.path.join(SavingPath, f'Indices_Split_{subject_range}.pkl')\n",
    "\n",
    "# training_saving_path = os.path.join(SavingPath, 'TransformedTraining.pkl')\n",
    "# testing_saving_path = os.path.join(SavingPath, 'TransformedTesting.pkl')\n",
    "\n",
    "# indices_saving_path = os.path.join(SavingPath, 'Indices_Split.pkl')\n",
    "\n",
    "\n",
    "# Save TransformedTraining\n",
    "with open(training_saving_path, 'wb') as file:\n",
    "    pickle.dump(TransformedTraining, file)\n",
    "\n",
    "# Save TransformedTesting\n",
    "with open(testing_saving_path, 'wb') as file:\n",
    "    pickle.dump(TransformedTesting, file)\n",
    "\n",
    "# Save Indices\n",
    "with open(indices_saving_path, 'wb') as file:\n",
    "    pickle.dump(indices_split, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e49441-5549-434a-be96-9635aa1698b7",
   "metadata": {},
   "source": [
    "# Extra/Old scripts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "871f4b95-eedd-4255-80bc-377c5536b38e",
   "metadata": {},
   "source": [
    "## 1. Recentre - recentres all trials to have centre of mass equal to identity matrix, followed by tangent space mapping (ref = I) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2c49f25-a08f-4870-b85f-82d540bd339c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Find covariance matrices\n",
    "cov_estimator = Covariances(estimator='lwf')\n",
    "Train_cov = cov_estimator.transform(X_train_dataset)\n",
    "Test_cov = cov_estimator.transform(X_test_dataset)\n",
    "\n",
    "# Find log-euclid centre ('M')\n",
    "Train_M = mean_logeuclid(Train_cov)\n",
    "Test_M = mean_logeuclid(Test_cov)\n",
    "\n",
    "# Centre and project to TS (& vectorise)\n",
    "Train_Centered = Recentering_TSProjection(Train_cov, Train_M)\n",
    "Test_Centered = Recentering_TSProjection(Test_cov, Test_M)\n",
    "\n",
    "#Sanity Check\n",
    "print(Train_Centered.shape)\n",
    "print(Test_Centered.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e3e53c8-f09a-481b-b999-3cc8781872ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Rescale - match matrix dispersion around mean in both 'source' and 'target' (setting the average norm within set to be 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ea5462c-09bb-4daf-a0cd-4ae8456e6999",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train_Rescale = Rescaling(Train_Centered)\n",
    "Test_Rescale = Rescaling(Test_Centered)\n",
    "\n",
    "#Sanity Check\n",
    "print(Train_Rescale.shape)\n",
    "print(Test_Rescale.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6efdca67-0ad3-4c39-9dc6-a23a203c2632",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Rotation (Alignment of 'target' vectors) - Align each mean of each class as much as possible (using Eulidea Procrustes procedure)\n",
    "### Once target vectors are aligned, can be used with models trained using 'Train_Rescale'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fcb6080-c352-4d3f-b8ab-17aa83cf6c31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calculate anchor points for each class \n",
    "Train_AnchorPoints=ClassMean(Train_Rescale, Y_train_dataset)\n",
    "Test_AnchorPoints=ClassMean(Test_Rescale, Y_test_dataset)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Anchor Points shape: \")\n",
    "print(Train_AnchorPoints.shape)\n",
    "print(Test_AnchorPoints.shape)\n",
    "\n",
    "# Cross-product matrix \n",
    "c_st = Train_AnchorPoints @ Test_AnchorPoints.T\n",
    "\n",
    "#Sanity check - should be (Nf, Nf)\n",
    "print(\"c_st shape: \")\n",
    "print(c_st.shape) \n",
    "\n",
    "# Perform Singular value decomposition on c_st\n",
    "U, D, VT = np.linalg.svd(c_st, full_matrices=False)\n",
    "\n",
    "# Find number of Nv vectors that explains 99.9% varaince \n",
    "explained_variance = D**2\n",
    "total_variance = np.sum(explained_variance)\n",
    "cumulative_explained_variance = np.cumsum(explained_variance) / total_variance\n",
    "Nv = np.argmax(cumulative_explained_variance >= 0.999) + 1  # +1 because of 0-based indexing\n",
    "\n",
    "U_tilde = U[:, :Nv] # Truncate using only Nv vectors\n",
    "VT_tilde = VT[:Nv, :]\n",
    "\n",
    "# Sanity check \n",
    "print(\"truncated U and VT: \")\n",
    "print(U_tilde.shape)\n",
    "print(VT_tilde.shape)\n",
    "\n",
    "print(Test_Rescale.shape)\n",
    "\n",
    "Nt = Test_Rescale.shape[0]\n",
    "Test_Rotated = np.zeros_like(Test_Rescale) # Initialise storage\n",
    "\n",
    "# Align each testing trial\n",
    "for t in range(Nt):\n",
    "    Test_Rotated[t] = U_tilde @ VT_tilde @ Test_Rescale[t]\n",
    "    \n",
    "print(\"Rotated Test: \")\n",
    "print(Test_Rotated.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyRiemann",
   "language": "python",
   "name": "pyriemann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
