{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e7debf-bbfd-49a5-89c3-211af6ca1203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.classification import TSclassifier\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.mean import mean_logeuclid\n",
    "#from pyriemann.classification import SVC\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import h5py\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53e10c7-1b31-4980-9ebf-da1828078ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5306ce-9a7b-4bd0-92d5-a28f25724bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subject_range = 'sub001_002'\n",
    "sub_ids = [0, 1]\n",
    "\n",
    "LoadingPath_Non='/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100_Individual/Delay/Dataset'\n",
    "\n",
    "Indices_Path = f'/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100_Individual/Delay/Transformed/Ind_Adaptive_TS_Transformed/Baseline/Indices_Split_{subject_range}.pkl'\n",
    "#Indices_Path = '/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100_Individual/Delay/Transformed/Ind_Adaptive_TS_Transformed/Baseline/Indices_Split_sub010_018.pkl'\n",
    "\n",
    "SavingPath = '/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100_Individual/Delay/Transformed/Ind_Adaptive_TS_Transformed/SpatFiltSignals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f0cf41-a089-4cb6-925d-7b9d9c38f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af047c8-de32-4e97-9afc-62a9c8149cff",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e1aa01-dde7-4b9e-a697-a372a8972e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def load_testing_data(testing_loading_path, test_subject_id, n_components=108):\n",
    "    \"\"\"\n",
    "    Load (raw)training and testing datasets for a given subject.\n",
    "    \n",
    "    Parameters:\n",
    "        loading_path (str): Path to the directory containing the dataset files.\n",
    "        subject_id (int): Subject ID (0-based index).\n",
    "        n_components (int): Number of components/features to extract.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing all loaded datasets and features.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    testing_dataset_path = os.path.join(testing_loading_path, 'TestingProjected.mat') \n",
    " \n",
    "    # Delay\n",
    "    with h5py.File(testing_dataset_path, 'r') as f_testing_dataset:\n",
    "            testing_dataset = f_testing_dataset['TestingProjected']\n",
    "            subject_cell_ref = testing_dataset[test_subject_id, 0]\n",
    "            subject_data = f_testing_dataset[subject_cell_ref]\n",
    "            X_test_dataset = np.transpose(subject_data['x'][:], (0, 2, 1))\n",
    "            Y_test_dataset = np.squeeze(subject_data['y'][:])\n",
    "\n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    data = {\n",
    "        \"X_test_dataset\": X_test_dataset,\n",
    "        \"Y_test_dataset\": Y_test_dataset,\n",
    "    }\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbf2f05a-6da4-4540-a2f9-a4f7c0eb94c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_spd(matrix):\n",
    "    \"\"\"Check if a matrix is Symmetric Positive Definite (SPD).\"\"\"\n",
    "    if not isinstance(matrix, np.ndarray):\n",
    "        return False\n",
    "    if matrix.shape[0] != matrix.shape[1]:\n",
    "        return False  # Not square\n",
    "    if not np.allclose(matrix, matrix.T, atol=1e-8):\n",
    "        return False  # Not symmetric\n",
    "    try:\n",
    "        # Try Cholesky decomposition\n",
    "        np.linalg.cholesky(matrix)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False  # Not positive definite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d899d92-27e4-40c7-b55a-60903f3160a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyriemann.utils.base import nearest_sym_pos_def\n",
    "from pyriemann.utils.test import is_sym_pos_def\n",
    "\n",
    "def ensure_spd(matrix): # Use in Recentering step \n",
    "    \"\"\"\n",
    "    Ensures a matrix is Symmetric Positive Definite (SPD). If not, projects it\n",
    "    to the nearest SPD matrix using PyRiemann utilities.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (ndarray): A 2D square matrix (Nc x Nc)\n",
    "\n",
    "    Returns:\n",
    "        ndarray: An SPD matrix (Nc x Nc)\n",
    "    \"\"\"\n",
    "    if is_sym_pos_def(matrix):\n",
    "        return matrix\n",
    "    else:\n",
    "        # Reshape to (1, Nc, Nc) for PyRiemann function\n",
    "        matrix_batch = matrix[np.newaxis, :, :]  # shape: (1, Nc, Nc)\n",
    "        nearest_spd = nearest_sym_pos_def(matrix_batch)  # output: (1, Nc, Nc)\n",
    "        return nearest_spd[0]  # shape: (Nc, Nc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4d346c-c4c0-4b8b-9204-0fcd0a24a026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def load_training_subject_data(training_loading_path, test_subject_id, train_subject_id, n_components=108):\n",
    "    \"\"\"\n",
    "    Load (raw)training and testing datasets for a given subject.\n",
    "    \n",
    "    Parameters:\n",
    "        loading_path (str): Path to the directory containing the dataset files.\n",
    "        subject_id (int): Subject ID (0-based index).\n",
    "        n_components (int): Number of components/features to extract.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing all loaded datasets and features.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    training_dataset_path = os.path.join(training_loading_path, 'TrainingProjected.mat') \n",
    " \n",
    "    with h5py.File(training_dataset_path, 'r') as f_training_dataset:\n",
    "        training_dataset = f_training_dataset['TrainingProjected']\n",
    "        training_cell_ref = training_dataset[test_subject_id,0]\n",
    "        training_cell = f_training_dataset[training_cell_ref]\n",
    "\n",
    "        # Access the specific train subject inside the nested 1x17 cell\n",
    "        subject_ref = training_cell[train_subject_id,0]\n",
    "        subject_data = f_training_dataset[subject_ref]\n",
    "\n",
    "        X_train_dataset = np.transpose(subject_data['x'][:], (0, 2, 1))\n",
    "        Y_train_dataset = np.squeeze(subject_data['y'][:])\n",
    " \n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    data = {\n",
    "        \"X_train_dataset\": X_train_dataset,\n",
    "        \"Y_train_dataset\": Y_train_dataset,\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe947fa-ef16-4608-bdc3-ef65a8b6b775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recentering function \n",
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from pyriemann.utils.tangentspace import tangent_space\n",
    "\n",
    "def Recentering_TSProjection(C, M):\n",
    "    \"\"\"\n",
    "    1. Centers each covariance matrix: M^(-1/2) C M^(-1/2))\n",
    "    2. Projects to TS (& vectorises) using identity matrix as 'new centre'\n",
    "    \n",
    "    Parameters:\n",
    "        C (ndarray): set of covariance matrices (Nt, Nc, Nc) \n",
    "        M (ndarray): Reference matrix (Taken as log-eulid mean)\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: The log-transformed matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    Nt, _, _ = C.shape # Number of trials \n",
    "    \n",
    "    M_inv_sqrt = fractional_matrix_power(M, -0.5) # Compute M^(-1/2)\n",
    "    \n",
    "    #Check M_inv_sqrt if SPD - if not, use PyRiemann pyriemann.utils.base.nearest_sym_pos_def\n",
    "    M_inv_sqrt = ensure_spd(M_inv_sqrt)\n",
    "    \n",
    "    #print(M_inv_sqrt.shape)\n",
    "    #print(is_sym_pos_def(M_inv_sqrt) )\n",
    "    \n",
    "    # 1. Recentering step (on each covariance matrix) \n",
    "    X_centered = np.array([M_inv_sqrt @ C[i] @ M_inv_sqrt for i in range(Nt)])\n",
    "    #print(X_centered.shape)\n",
    "    \n",
    "    #2. TS projection \n",
    "    Centered_M = np.eye(C.shape[1]) # New center after centering is the identity matrix \n",
    "    X_TS = tangent_space(X_centered, Centered_M) # Project to TS (& vectorise)\n",
    "\n",
    "    \n",
    "    return X_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ecc920-a2d3-41eb-99da-5c9974d79fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Rescaling(C):\n",
    "    \"\"\"\n",
    "    Rescale all tangent vectors: c_tilde = c / (1/N_t * sum_n ||c_n||)\n",
    "\n",
    "    Parameters:\n",
    "        C (ndarray): Collection of tangent vectors (c_n) to be rescaled\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Rescaled tangents vectors `c_tilde`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if C contains any 'nan' and replace with 0.0\n",
    "    if np.isnan(C).any():\n",
    "        C = np.nan_to_num(C, nan=0.0)\n",
    "    \n",
    "    Nt = C.shape[0] # Number of vectors\n",
    "    \n",
    "    # Compute the average magnitude (1/N_t * sum(||c_n||))\n",
    "    avg_magnitude = np.sum(np.linalg.norm(C, axis=1)) / Nt\n",
    "\n",
    "    # Normalize all vectors simultaneously\n",
    "    c_tilde  = C / avg_magnitude\n",
    "\n",
    "    return c_tilde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa67384-2202-45f6-8706-e155c6ec802d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rotation Functions\n",
    "\n",
    "\n",
    "# Mean per class \n",
    "def ClassMean(Dataset, Labels):\n",
    "    \"\"\"\n",
    "    Compute the class-wise mean based on the provided equation:\n",
    "        s̄_k = (1 / N_k) * Σ s̃_i  (for y_i = k)\n",
    "    \n",
    "    Parameters:\n",
    "        Dataset (ndarray): Tangent vectors, Nt = number of trials, Nf = features per trial.\n",
    "        Labels (ndarray): The corresponding labels for the dataset, of shape (Nt,).\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: matrix of shape (k, Nf) (k = number of classes)\n",
    "    \"\"\"\n",
    "    unique_classes = np.unique(Labels)  # Find unique classes \n",
    "    class_means = []\n",
    "\n",
    "    for cls in unique_classes: # for each class\n",
    "        # Select data corresponding to the current class\n",
    "        class_data = Dataset[Labels == cls]  # Filter rows where label == cls\n",
    "        N_k = class_data.shape[0]  # Number of trials for class k\n",
    "        \n",
    "        # Compute mean\n",
    "        class_mean = np.mean(class_data, axis=0)\n",
    "        \n",
    "        # Store result\n",
    "        class_means.append(class_mean)\n",
    "    \n",
    "    C_bar = np.column_stack(class_means)\n",
    "\n",
    "    return C_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025eb33c-6b6d-4282-b566-4de485ebf87a",
   "metadata": {},
   "source": [
    "# Load subject's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34735802-8e50-41b9-bd3a-bea886aaaaae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing subject 0:\n",
      "Loaded testing datatsets shape: \n",
      "(3278, 108, 100)\n",
      "Shape of test adaptive data\n",
      "(1617, 108, 100)\n",
      "Labels sizes: Training (50538,), Testing (3278,)\n",
      "Centred shapes: \n",
      "(50538, 5886)\n",
      "(3278, 5886)\n",
      "Rescaled shapes: \n",
      "(50538, 5886)\n",
      "(3278, 5886)\n",
      "Fold index: 0\n",
      "Indices for calibration subset: [   2    5    6 ... 3272 3274 3277]\n",
      "Indices for testing subset: [   0    1    3 ... 3273 3275 3276]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 1)\n",
      "(1, 5886)\n",
      "(1639, 5886)\n",
      "Rotated Test: \n",
      "(1639, 5886)\n",
      "Fold index: 1\n",
      "Indices for calibration subset: [   0    1    3 ... 3273 3275 3276]\n",
      "Indices for testing subset: [   2    5    6 ... 3272 3274 3277]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 1)\n",
      "(1, 5886)\n",
      "(1639, 5886)\n",
      "Rotated Test: \n",
      "(1639, 5886)\n",
      "Testing subject 1:\n",
      "Loaded testing datatsets shape: \n",
      "(3055, 108, 100)\n",
      "Shape of test adaptive data\n",
      "(1498, 108, 100)\n",
      "Labels sizes: Training (50761,), Testing (3055,)\n",
      "Centred shapes: \n",
      "(50761, 5886)\n",
      "(3055, 5886)\n",
      "Rescaled shapes: \n",
      "(50761, 5886)\n",
      "(3055, 5886)\n",
      "Fold index: 0\n",
      "Indices for calibration subset: [   0    1    2 ... 3048 3052 3054]\n",
      "Indices for testing subset: [   8    9   14 ... 3050 3051 3053]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 1)\n",
      "(1, 5886)\n",
      "(1527, 5886)\n",
      "Rotated Test: \n",
      "(1528, 5886)\n",
      "Fold index: 1\n",
      "Indices for calibration subset: [   8    9   14 ... 3050 3051 3053]\n",
      "Indices for testing subset: [   0    1    2 ... 3048 3052 3054]\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 1)\n",
      "(1, 5886)\n",
      "(1528, 5886)\n",
      "Rotated Test: \n",
      "(1527, 5886)\n"
     ]
    }
   ],
   "source": [
    "TransformedTraining = {}\n",
    "TransformedTesting = {}\n",
    "indices_split = {}\n",
    "\n",
    "#for test_sub_id in [0]:\n",
    "#for test_sub_id in range(18):\n",
    "\n",
    "# sub001 - 004: (0, 4)\n",
    "# sub005 - 009: (4, 9) #sub005-007: (4, 7) and sub008-009: (7, 9)\n",
    "# sub010 - 014: (9, 14) #sub010 - 012: (9, 12) and sub013-014: (12, 14)\n",
    "# sub015-018: (14, 18) 'sub015-016: (14, 16) and sub017-018: (16, 18)\n",
    "\n",
    "for test_sub_id in sub_ids:\n",
    "    print(f\"Testing subject {test_sub_id}:\")\n",
    "    test_subject_data = load_testing_data(LoadingPath_Non, test_sub_id, n_components=Nc)\n",
    "\n",
    "\n",
    "    X_test_dataset = test_subject_data[\"X_test_dataset\"]\n",
    "    Y_test_dataset = test_subject_data[\"Y_test_dataset\"]\n",
    "    \n",
    "    \n",
    "    # Sanity check\n",
    "    print(\"Loaded testing datatsets shape: \")\n",
    "    print(X_test_dataset.shape)\n",
    "    \n",
    "# ---------------------- 1. Recentre (TESTING) - recentres all trials to have centre of mass equal to identity matrix, followed by tangent space mapping (ref = I) ----------------------     \n",
    "    # Find Adaptive trials \n",
    "    data = X_test_dataset[Y_test_dataset == 1]\n",
    "    print(\"Shape of test adaptive data\")\n",
    "    print(data.shape)\n",
    "\n",
    "    mean = np.mean(data, axis=0)  # Shape: (Nc, Ns)\n",
    "    std = np.std(data, axis=0)    # Shape: (Nc, Ns)\n",
    "\n",
    "\n",
    "    # Avoid division by zero\n",
    "    epsilon = 1e-10\n",
    "    std = np.maximum(std, epsilon)\n",
    "\n",
    "    # Standardize data\n",
    "    X_test_standardized = (data - mean) / std\n",
    "    \n",
    "    # Find covariance matrices\n",
    "    cov_estimator = Covariances(estimator='lwf')\n",
    "    Test_Adaptive_cov = cov_estimator.transform(X_test_standardized)\n",
    "\n",
    "    # Find log-euclid centre ('M')\n",
    "    Test_M = mean_logeuclid(Test_Adaptive_cov)\n",
    "\n",
    "    # Centre and project to TS (& vectorise)\n",
    "    Test_cov = cov_estimator.transform(X_test_dataset)\n",
    "    Test_Centered = Recentering_TSProjection(Test_cov, Test_M)\n",
    "\n",
    "    # ---------------------- 1. Recentre (TRAINING) - recentres all trials to have centre of mass equal to identity matrix, followed by tangent space mapping (ref = I) ----------------------     \n",
    "    Train_Centered_list = []\n",
    "    Y_train_list=[]\n",
    "    \n",
    "    # Load each (ind) tranining dataset\n",
    "    for train_sub_id in range(17):\n",
    "        #print(f\"Training subject {train_sub_id}:\")\n",
    "        train_subject_data = load_training_subject_data(LoadingPath_Non, test_sub_id, train_sub_id, n_components=Nc)\n",
    "\n",
    "\n",
    "        X_train_dataset = train_subject_data[\"X_train_dataset\"]\n",
    "        Y_train_sub_dataset = train_subject_data[\"Y_train_dataset\"]\n",
    "    \n",
    "\n",
    "        # Sanity check\n",
    "        # print(\"Loaded training datatsets shape: \")\n",
    "        # print(X_train_dataset.shape)\n",
    "        \n",
    "        # Find Adaptive trials\n",
    "        data = X_train_dataset[Y_train_sub_dataset == 1]\n",
    "        # print(\"Shape of adaptive data\")\n",
    "        # print(data.shape)\n",
    "\n",
    "        mean = np.mean(data, axis=0)  # Shape: (Nc, Ns)\n",
    "        std = np.std(data, axis=0)    # Shape: (Nc, Ns)\n",
    "\n",
    "\n",
    "        # Avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        std = np.maximum(std, epsilon)\n",
    "\n",
    "        # Standardize data\n",
    "        X_train_standardized = (data - mean) / std\n",
    "\n",
    "        \n",
    "        # Find covariance matrices\n",
    "        cov_estimator = Covariances(estimator='lwf')\n",
    "        Train_Adaptive_cov = cov_estimator.transform(X_train_standardized)\n",
    "\n",
    "        # Find log-euclid centre ('M')\n",
    "        Train_M = mean_logeuclid(Train_Adaptive_cov)\n",
    "\n",
    "        # Centre and project to TS (& vectorise)\n",
    "        Train_cov = cov_estimator.transform(X_train_dataset)\n",
    "        Train_sub_Centered = Recentering_TSProjection(Train_cov, Train_M)\n",
    "        \n",
    "        # Concatenate each subjects' centred vector and corresponding labels \n",
    "        Train_Centered_list.append(Train_sub_Centered)  \n",
    "        Y_train_list.append(Y_train_sub_dataset)\n",
    "        \n",
    "    Train_Centered = np.concatenate(Train_Centered_list, axis=0)\n",
    "    Y_train_dataset = np.concatenate(Y_train_list, axis = 0)\n",
    "    \n",
    "    print(f\"Labels sizes: Training {Y_train_dataset.shape}, Testing {Y_test_dataset.shape}\")\n",
    "    \n",
    "    #Sanity Check\n",
    "    print(\"Centred shapes: \")\n",
    "    print(Train_Centered.shape)\n",
    "    print(Test_Centered.shape)\n",
    "    \n",
    "# ---------------------- 2. Rescale - match matrix dispersion around mean in both 'source' and 'target' (setting the average norm within set to be 1) ----------------------\n",
    "    Train_Rescale = Rescaling(Train_Centered)\n",
    "    Test_Rescale = Rescaling(Test_Centered)\n",
    "\n",
    "    #Sanity Check\n",
    "    print(\"Rescaled shapes: \")\n",
    "    print(Train_Rescale.shape)\n",
    "    print(Test_Rescale.shape)\n",
    "\n",
    "# ---------------------- 3. Rotation (Alignment of 'target' vectors) - Align each mean of each class as much as possible (using Eulidea Procrustes procedure)  ----------------------\n",
    "# ---------------------- Once target vectors are aligned, can be used with models trained using 'Train_Rescale' ---------------------- \n",
    "# Calculate anchor points for each class \n",
    "# Split Testing into calibration and testing \n",
    "    Test_Rotated_list=[]\n",
    "    Test_labels_list=[]\n",
    "    \n",
    "\n",
    "    #Instead of creating 'new' split - use same split as in 'Baseline' to ensure corresponding trials are used in same calibration:testing split\n",
    "    with open(Indices_Path, 'rb') as f:\n",
    "        indices_split = pickle.load(f)\n",
    "\n",
    "    \n",
    "    for fold_idx in range(2):\n",
    "    \n",
    "        print(f\"Fold index: {fold_idx}\")\n",
    "        fold_key = f\"Fold{fold_idx + 1}\"\n",
    "        \n",
    "        # Load calibration and test idx\n",
    "        calibration_idx = indices_split[test_sub_id][fold_key][\"calibration_idx\"]\n",
    "        test_idx = indices_split[test_sub_id][fold_key][\"test_idx\"]\n",
    "        \n",
    "        print(f\"Indices for calibration subset: {calibration_idx}\")\n",
    "        print(f\"Indices for testing subset: {test_idx}\")\n",
    "\n",
    "        X_calibration, X_test = Test_Rescale[calibration_idx], Test_Rescale[test_idx]\n",
    "        Y_calibration, Y_test = Y_test_dataset[calibration_idx], Y_test_dataset[test_idx]\n",
    "        \n",
    "\n",
    "        Train_AnchorPoints=ClassMean(Train_Rescale, Y_train_dataset)\n",
    "        Test_AnchorPoints=ClassMean(X_calibration, Y_calibration) # Only calibration subset\n",
    "\n",
    "        # Sanity check\n",
    "        print(\"Anchor Points shape: \")\n",
    "        print(Train_AnchorPoints.shape)\n",
    "        print(Test_AnchorPoints.shape)\n",
    "\n",
    "        # Cross-product matrix \n",
    "        c_st = Train_AnchorPoints @ Test_AnchorPoints.T\n",
    "\n",
    "        #Sanity check - should be (Nf, Nf)\n",
    "        print(\"c_st shape: \")\n",
    "        print(c_st.shape) \n",
    "\n",
    "        # Perform Singular value decomposition on c_st\n",
    "        U, D, VT = np.linalg.svd(c_st, full_matrices=False)\n",
    "\n",
    "        # Find number of Nv vectors that explains 99.9% varaince \n",
    "        explained_variance = D**2\n",
    "        total_variance = np.sum(explained_variance)\n",
    "        cumulative_explained_variance = np.cumsum(explained_variance) / total_variance\n",
    "        Nv = np.argmax(cumulative_explained_variance >= 0.999) + 1  # +1 because of 0-based indexing\n",
    "\n",
    "        U_tilde = U[:, :Nv] # Truncate using only Nv vectors\n",
    "        VT_tilde = VT[:Nv, :]\n",
    "\n",
    "        # Sanity check \n",
    "        print(\"truncated U and VT: \")\n",
    "        print(U_tilde.shape)\n",
    "        print(VT_tilde.shape)\n",
    "\n",
    "        print(X_calibration.shape)\n",
    "\n",
    "        Nt = X_test.shape[0]\n",
    "        Test_Rotated = np.zeros_like(X_test) # Initialise storage\n",
    "\n",
    "        # Align each testing trial\n",
    "        for t in range(Nt):\n",
    "            Test_Rotated[t] = U_tilde @ VT_tilde @ X_test[t]\n",
    "\n",
    "        print(\"Rotated Test: \")\n",
    "        print(Test_Rotated.shape)\n",
    "\n",
    "         # Save for this fold\n",
    "        Test_Rotated_list.append(Test_Rotated)\n",
    "        Test_labels_list.append(Y_test)\n",
    "\n",
    "        if test_sub_id not in TransformedTesting:\n",
    "            TransformedTesting[test_sub_id] = {}\n",
    "\n",
    "        TransformedTesting[test_sub_id][f\"Fold{fold_idx + 1}\"] = {\n",
    "            \"x\": Test_Rotated,\n",
    "            \"y\": Y_test\n",
    "        }\n",
    "\n",
    "    if test_sub_id not in TransformedTraining:\n",
    "        TransformedTraining[test_sub_id] = {}\n",
    "        \n",
    "    # Store per subject (transformed training and testing datasets)\n",
    "    TransformedTraining[test_sub_id] = {\n",
    "        \"x\": Train_Rescale,\n",
    "        \"y\": Y_train_dataset\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "263b96b8-f07c-4c03-82d7-5139d84da580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 15.642222026983896 minutes\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90cd12b5-a5d7-42d6-9b42-96d86d70635e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100_Individual/Delay/Transformed/Ind_Adaptive_TS_Transformed/SpatFiltSignals'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SavingPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e46fbc-3f5f-4dd8-9e0d-afbf34f16176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define saving paths for the variables\n",
    "training_saving_path = os.path.join(SavingPath, f'TransformedTraining_{subject_range}.pkl')\n",
    "testing_saving_path = os.path.join(SavingPath, f'TransformedTesting_{subject_range}.pkl')\n",
    "\n",
    "\n",
    "# Save TransformedTraining\n",
    "with open(training_saving_path, 'wb') as file:\n",
    "    pickle.dump(TransformedTraining, file)\n",
    "\n",
    "# Save TransformedTesting\n",
    "with open(testing_saving_path, 'wb') as file:\n",
    "    pickle.dump(TransformedTesting, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyRiemann",
   "language": "python",
   "name": "pyriemann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
