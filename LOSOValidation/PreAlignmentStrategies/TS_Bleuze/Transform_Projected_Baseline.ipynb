{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e7debf-bbfd-49a5-89c3-211af6ca1203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.classification import TSclassifier\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.mean import mean_logeuclid\n",
    "#from pyriemann.classification import SVC\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import h5py\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53e10c7-1b31-4980-9ebf-da1828078ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5306ce-9a7b-4bd0-92d5-a28f25724bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LoadingPath_Non='/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100/Delay'\n",
    "\n",
    "SavingPath = '/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100/Delay/Transformed/TangentSpace_PS/Transformed/SpatFiltSignals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f0cf41-a089-4cb6-925d-7b9d9c38f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af047c8-de32-4e97-9afc-62a9c8149cff",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56e279f-f83d-44b6-95c0-1ffebee10345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def load_subject_data(loading_path, subject_id, n_components=108):\n",
    "    \"\"\"\n",
    "    Load (raw)training and testing datasets for a given subject.\n",
    "    \n",
    "    Parameters:\n",
    "        loading_path (str): Path to the directory containing the dataset files.\n",
    "        subject_id (int): Subject ID (0-based index).\n",
    "        n_components (int): Number of components/features to extract.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing all loaded datasets and features.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    training_projected_path = os.path.join(loading_path, 'TrainingProjected_sub017_018.mat')\n",
    "    testing_projected_path = os.path.join(loading_path, 'TestingProjected_sub017_018.mat')\n",
    "\n",
    "    # Load Training Projected Dataset\n",
    "    with h5py.File(training_projected_path, 'r') as f_training_projected:\n",
    "        training_projected = f_training_projected['TrainingProjected']\n",
    "        subject_cell_ref = training_projected[subject_id, 0]\n",
    "        subject_data = f_training_projected[subject_cell_ref]\n",
    "        X_train_projected = np.transpose(subject_data['x'][:], (0, 2, 1))\n",
    "        Y_train_projected = np.squeeze(subject_data['y'][:])\n",
    "\n",
    "    # Load Testing Projected Dataset\n",
    "    # Advance\n",
    "    # f_testing_projected = scipy.io.loadmat(testing_projected_path)\n",
    "    # testing_projected = f_testing_projected['TestingProjected'][0, subject_id]\n",
    "    # X_test_projected = np.transpose(np.array(testing_projected['x'][0][0]), (2, 0, 1))\n",
    "    # #X_test_projected = np.abs(X_test_projected) #Added pre-aligment data is complex\n",
    "    # Y_test_projected = np.squeeze(np.array(testing_projected['y'][0][0]))\n",
    "    \n",
    "    # Delay\n",
    "    with h5py.File(testing_projected_path, 'r') as f_testing_projected:\n",
    "        testing_projected = f_testing_projected['TestingProjected']\n",
    "        subject_cell_ref = testing_projected[subject_id, 0]\n",
    "        subject_data = f_testing_projected[subject_cell_ref]\n",
    "        X_test_projected = np.transpose(subject_data['x'][:], (0, 2, 1))\n",
    "        Y_test_projected = np.squeeze(subject_data['y'][:])\n",
    "\n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    data = {\n",
    "        \"X_train_projected\": X_train_projected,\n",
    "        \"Y_train_projected\": Y_train_projected,\n",
    "        \"X_test_projected\": X_test_projected,\n",
    "        \"Y_test_projected\": Y_test_projected,\n",
    "    }\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe947fa-ef16-4608-bdc3-ef65a8b6b775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recentering function \n",
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from pyriemann.utils.tangentspace import tangent_space\n",
    "\n",
    "def Recentering_TSProjection(C, M):\n",
    "    \"\"\"\n",
    "    1. Centers each covariance matrix: M^(-1/2) C M^(-1/2))\n",
    "    2. Projects to TS (& vectorises) using identity matrix as 'new centre'\n",
    "    \n",
    "    Parameters:\n",
    "        C (ndarray): set of covariance matrices (Nt, Nc, Nc) \n",
    "        M (ndarray): Reference matrix (Taken as log-eulid mean)\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: The log-transformed matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    Nt, _, _ = C.shape # Number of trials \n",
    "    \n",
    "    M_inv_sqrt = fractional_matrix_power(M, -0.5) # Compute M^(-1/2)\n",
    "    \n",
    "    # 1. Recentering step (on each covariance matrix) \n",
    "    X_centered = np.array([M_inv_sqrt @ C[i] @ M_inv_sqrt for i in range(Nt)])\n",
    "    #print(X_centered.shape)\n",
    "    \n",
    "    #2. TS projection \n",
    "    Centered_M = np.eye(C.shape[1]) # New center after centering is the identity matrix \n",
    "    X_TS = tangent_space(X_centered, Centered_M) # Project to TS (& vectorise)\n",
    "\n",
    "    \n",
    "    return X_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ecc920-a2d3-41eb-99da-5c9974d79fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Rescaling(C):\n",
    "    \"\"\"\n",
    "    Rescale all tangent vectors: c_tilde = c / (1/N_t * sum_n ||c_n||)\n",
    "\n",
    "    Parameters:\n",
    "        C (ndarray): Collection of tangent vectors (c_n) to be rescaled\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Rescaled tangents vectors `c_tilde`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if C contains any 'nan' and replace with 0.0\n",
    "    if np.isnan(C).any():\n",
    "        C = np.nan_to_num(C, nan=0.0)\n",
    "    \n",
    "    Nt = C.shape[0] # Number of vectors\n",
    "    \n",
    "    # Compute the average magnitude (1/N_t * sum(||c_n||))\n",
    "    avg_magnitude = np.sum(np.linalg.norm(C, axis=1)) / Nt\n",
    "\n",
    "    # Normalize all vectors simultaneously\n",
    "    c_tilde  = C / avg_magnitude\n",
    "\n",
    "    return c_tilde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa67384-2202-45f6-8706-e155c6ec802d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rotation Functions\n",
    "\n",
    "\n",
    "# Mean per class \n",
    "def ClassMean(Dataset, Labels):\n",
    "    \"\"\"\n",
    "    Compute the class-wise mean based on the provided equation:\n",
    "        s̄_k = (1 / N_k) * Σ s̃_i  (for y_i = k)\n",
    "    \n",
    "    Parameters:\n",
    "        Dataset (ndarray): Tangent vectors, Nt = number of trials, Nf = features per trial.\n",
    "        Labels (ndarray): The corresponding labels for the dataset, of shape (Nt,).\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: matrix of shape (k, Nf) (k = number of classes)\n",
    "    \"\"\"\n",
    "    unique_classes = np.unique(Labels)  # Find unique classes \n",
    "    class_means = []\n",
    "\n",
    "    for cls in unique_classes: # for each class\n",
    "        # Select data corresponding to the current class\n",
    "        class_data = Dataset[Labels == cls]  # Filter rows where label == cls\n",
    "        N_k = class_data.shape[0]  # Number of trials for class k\n",
    "        \n",
    "        # Compute mean\n",
    "        class_mean = np.mean(class_data, axis=0)\n",
    "        \n",
    "        # Store result\n",
    "        class_means.append(class_mean)\n",
    "    \n",
    "    C_bar = np.column_stack(class_means)\n",
    "\n",
    "    return C_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025eb33c-6b6d-4282-b566-4de485ebf87a",
   "metadata": {},
   "source": [
    "# Load subject's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34735802-8e50-41b9-bd3a-bea886aaaaae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16:\n",
      "Loaded datatsets shape: \n",
      "(51991, 108, 100)\n",
      "(1825, 108, 100)\n",
      "Centred shapes: \n",
      "(51991, 5886)\n",
      "(1825, 5886)\n",
      "Rescaled shapes: \n",
      "(51991, 5886)\n",
      "(1825, 5886)\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(1825, 5886)\n",
      "Rotated Test: \n",
      "(1825, 5886)\n",
      "Subject 17:\n",
      "Loaded datatsets shape: \n",
      "(51394, 108, 100)\n",
      "(2422, 108, 100)\n",
      "Centred shapes: \n",
      "(51394, 5886)\n",
      "(2422, 5886)\n",
      "Rescaled shapes: \n",
      "(51394, 5886)\n",
      "(2422, 5886)\n",
      "Anchor Points shape: \n",
      "(5886, 2)\n",
      "(5886, 2)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(2422, 5886)\n",
      "Rotated Test: \n",
      "(2422, 5886)\n"
     ]
    }
   ],
   "source": [
    "TransformedTraining = {}\n",
    "TransformedTesting = {}\n",
    "\n",
    "for sub_id in [16, 17]:\n",
    "    print(f\"Subject {sub_id}:\")\n",
    "    subject_data = load_subject_data(LoadingPath_Non, sub_id, n_components=Nc)\n",
    "\n",
    "    X_train_dataset = subject_data[\"X_train_projected\"]\n",
    "    Y_train_dataset = subject_data[\"Y_train_projected\"]\n",
    "    X_test_dataset = subject_data[\"X_test_projected\"]\n",
    "    Y_test_dataset = subject_data[\"Y_test_projected\"]\n",
    "\n",
    "    # Sanity check\n",
    "    print(\"Loaded datatsets shape: \")\n",
    "    print(X_train_dataset.shape)\n",
    "    print(X_test_dataset.shape)\n",
    "    \n",
    "# ---------------------- 1. Recentre - recentres all trials to have centre of mass equal to identity matrix, followed by tangent space mapping (ref = I) ---------------------- \n",
    "    # Find covariance matrices\n",
    "    cov_estimator = Covariances(estimator='lwf')\n",
    "    Train_cov = cov_estimator.transform(X_train_dataset)\n",
    "    Test_cov = cov_estimator.transform(X_test_dataset)\n",
    "\n",
    "    # Find log-euclid centre ('M')\n",
    "    Train_M = mean_logeuclid(Train_cov)\n",
    "    Test_M = mean_logeuclid(Test_cov)\n",
    "\n",
    "    # Centre and project to TS (& vectorise)\n",
    "    Train_Centered = Recentering_TSProjection(Train_cov, Train_M)\n",
    "    Test_Centered = Recentering_TSProjection(Test_cov, Test_M)\n",
    "\n",
    "    #Sanity Check\n",
    "    print(\"Centred shapes: \")\n",
    "    print(Train_Centered.shape)\n",
    "    print(Test_Centered.shape)\n",
    "\n",
    "# ---------------------- 2. Rescale - match matrix dispersion around mean in both 'source' and 'target' (setting the average norm within set to be 1) ----------------------\n",
    "    Train_Rescale = Rescaling(Train_Centered)\n",
    "    Test_Rescale = Rescaling(Test_Centered)\n",
    "\n",
    "    #Sanity Check\n",
    "    print(\"Rescaled shapes: \")\n",
    "    print(Train_Rescale.shape)\n",
    "    print(Test_Rescale.shape)\n",
    "\n",
    "# ---------------------- 3. Rotation (Alignment of 'target' vectors) - Align each mean of each class as much as possible (using Eulidea Procrustes procedure)  ----------------------\n",
    "# ---------------------- Once target vectors are aligned, can be used with models trained using 'Train_Rescale' ---------------------- \n",
    "# Calculate anchor points for each class \n",
    "    Train_AnchorPoints=ClassMean(Train_Rescale, Y_train_dataset)\n",
    "    Test_AnchorPoints=ClassMean(Test_Rescale, Y_test_dataset)\n",
    "\n",
    "    # Sanity check\n",
    "    print(\"Anchor Points shape: \")\n",
    "    print(Train_AnchorPoints.shape)\n",
    "    print(Test_AnchorPoints.shape)\n",
    "\n",
    "    # Cross-product matrix \n",
    "    c_st = Train_AnchorPoints @ Test_AnchorPoints.T\n",
    "\n",
    "    #Sanity check - should be (Nf, Nf)\n",
    "    print(\"c_st shape: \")\n",
    "    print(c_st.shape) \n",
    "\n",
    "    # Perform Singular value decomposition on c_st\n",
    "    U, D, VT = np.linalg.svd(c_st, full_matrices=False)\n",
    "\n",
    "    # Find number of Nv vectors that explains 99.9% varaince \n",
    "    explained_variance = D**2\n",
    "    total_variance = np.sum(explained_variance)\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance) / total_variance\n",
    "    Nv = np.argmax(cumulative_explained_variance >= 0.999) + 1  # +1 because of 0-based indexing\n",
    "\n",
    "    U_tilde = U[:, :Nv] # Truncate using only Nv vectors\n",
    "    VT_tilde = VT[:Nv, :]\n",
    "\n",
    "    # Sanity check \n",
    "    print(\"truncated U and VT: \")\n",
    "    print(U_tilde.shape)\n",
    "    print(VT_tilde.shape)\n",
    "\n",
    "    print(Test_Rescale.shape)\n",
    "\n",
    "    Nt = Test_Rescale.shape[0]\n",
    "    Test_Rotated = np.zeros_like(Test_Rescale) # Initialise storage\n",
    "\n",
    "    # Align each testing trial\n",
    "    for t in range(Nt):\n",
    "        Test_Rotated[t] = U_tilde @ VT_tilde @ Test_Rescale[t]\n",
    "\n",
    "    print(\"Rotated Test: \")\n",
    "    print(Test_Rotated.shape)\n",
    "    \n",
    "    # Store per subject (transformed training and testing datasets)\n",
    "    TransformedTraining[sub_id] = Train_Rescale\n",
    "    TransformedTesting[sub_id] = Test_Rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263b96b8-f07c-4c03-82d7-5139d84da580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 12.577589960892995 minutes\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90cd12b5-a5d7-42d6-9b42-96d86d70635e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100/Delay/Transformed/TangentSpace_PS/Transformed/SpatFiltSignals'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SavingPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e27f2f-e926-443a-965e-e9cf874ce3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define saving paths for the variables\n",
    "training_saving_path = os.path.join(SavingPath, 'TransformedTraining_sub017_018.pkl')\n",
    "testing_saving_path = os.path.join(SavingPath, 'TransformedTesting_sub017_018.pkl')\n",
    "\n",
    "# Save TransformedTraining\n",
    "with open(training_saving_path, 'wb') as file:\n",
    "    pickle.dump(TransformedTraining, file)\n",
    "\n",
    "# Save TransformedTesting\n",
    "with open(testing_saving_path, 'wb') as file:\n",
    "    pickle.dump(TransformedTesting, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e49441-5549-434a-be96-9635aa1698b7",
   "metadata": {},
   "source": [
    "# Extra/Old scripts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "871f4b95-eedd-4255-80bc-377c5536b38e",
   "metadata": {},
   "source": [
    "## 1. Recentre - recentres all trials to have centre of mass equal to identity matrix, followed by tangent space mapping (ref = I) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2c49f25-a08f-4870-b85f-82d540bd339c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Find covariance matrices\n",
    "cov_estimator = Covariances(estimator='lwf')\n",
    "Train_cov = cov_estimator.transform(X_train_dataset)\n",
    "Test_cov = cov_estimator.transform(X_test_dataset)\n",
    "\n",
    "# Find log-euclid centre ('M')\n",
    "Train_M = mean_logeuclid(Train_cov)\n",
    "Test_M = mean_logeuclid(Test_cov)\n",
    "\n",
    "# Centre and project to TS (& vectorise)\n",
    "Train_Centered = Recentering_TSProjection(Train_cov, Train_M)\n",
    "Test_Centered = Recentering_TSProjection(Test_cov, Test_M)\n",
    "\n",
    "#Sanity Check\n",
    "print(Train_Centered.shape)\n",
    "print(Test_Centered.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e3e53c8-f09a-481b-b999-3cc8781872ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Rescale - match matrix dispersion around mean in both 'source' and 'target' (setting the average norm within set to be 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ea5462c-09bb-4daf-a0cd-4ae8456e6999",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train_Rescale = Rescaling(Train_Centered)\n",
    "Test_Rescale = Rescaling(Test_Centered)\n",
    "\n",
    "#Sanity Check\n",
    "print(Train_Rescale.shape)\n",
    "print(Test_Rescale.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6efdca67-0ad3-4c39-9dc6-a23a203c2632",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Rotation (Alignment of 'target' vectors) - Align each mean of each class as much as possible (using Eulidea Procrustes procedure)\n",
    "### Once target vectors are aligned, can be used with models trained using 'Train_Rescale'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fcb6080-c352-4d3f-b8ab-17aa83cf6c31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calculate anchor points for each class \n",
    "Train_AnchorPoints=ClassMean(Train_Rescale, Y_train_dataset)\n",
    "Test_AnchorPoints=ClassMean(Test_Rescale, Y_test_dataset)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Anchor Points shape: \")\n",
    "print(Train_AnchorPoints.shape)\n",
    "print(Test_AnchorPoints.shape)\n",
    "\n",
    "# Cross-product matrix \n",
    "c_st = Train_AnchorPoints @ Test_AnchorPoints.T\n",
    "\n",
    "#Sanity check - should be (Nf, Nf)\n",
    "print(\"c_st shape: \")\n",
    "print(c_st.shape) \n",
    "\n",
    "# Perform Singular value decomposition on c_st\n",
    "U, D, VT = np.linalg.svd(c_st, full_matrices=False)\n",
    "\n",
    "# Find number of Nv vectors that explains 99.9% varaince \n",
    "explained_variance = D**2\n",
    "total_variance = np.sum(explained_variance)\n",
    "cumulative_explained_variance = np.cumsum(explained_variance) / total_variance\n",
    "Nv = np.argmax(cumulative_explained_variance >= 0.999) + 1  # +1 because of 0-based indexing\n",
    "\n",
    "U_tilde = U[:, :Nv] # Truncate using only Nv vectors\n",
    "VT_tilde = VT[:Nv, :]\n",
    "\n",
    "# Sanity check \n",
    "print(\"truncated U and VT: \")\n",
    "print(U_tilde.shape)\n",
    "print(VT_tilde.shape)\n",
    "\n",
    "print(Test_Rescale.shape)\n",
    "\n",
    "Nt = Test_Rescale.shape[0]\n",
    "Test_Rotated = np.zeros_like(Test_Rescale) # Initialise storage\n",
    "\n",
    "# Align each testing trial\n",
    "for t in range(Nt):\n",
    "    Test_Rotated[t] = U_tilde @ VT_tilde @ Test_Rescale[t]\n",
    "    \n",
    "print(\"Rotated Test: \")\n",
    "print(Test_Rotated.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyRiemann",
   "language": "python",
   "name": "pyriemann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
