{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cdbbc2-1614-4ee9-89dd-66c8813b4270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.classification import TSclassifier\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.mean import mean_logeuclid\n",
    "#from pyriemann.classification import SVC\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import h5py\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d05b49-406b-4d89-9c14-f7f5a8727794",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1507f8f1-d15d-41f3-bff7-2cf9c21a9398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LoadingPath_Non='/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100/Advance/Transformed/TangentSpace_PS/Raw'\n",
    "SavingPath = '/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100/Advance/Transformed/TangentSpace_PS/Transformed/Baseline/Clusters_Cst'\n",
    "#subject_ids = [0, 1, 2] \n",
    "#subject_ids = [9, 10, 11, 12, 13, 14, 15, 16, 17] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4d10a5-4b60-4573-971a-a5327ced20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d9ff1-af8d-475f-8900-98148ce310ac",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba22b9ca-1747-4624-bc97-369eda1613ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def load_subject_data(loading_path, subject_id, n_components=108):\n",
    "    \"\"\"\n",
    "    Load (raw)training and testing datasets for a given subject.\n",
    "    \n",
    "    Parameters:\n",
    "        loading_path (str): Path to the directory containing the dataset files.\n",
    "        subject_id (int): Subject ID (0-based index).\n",
    "        n_components (int): Number of components/features to extract.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing all loaded datasets and features.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    training_dataset_path = os.path.join(loading_path, 'TrainingDataset.mat')\n",
    "    testing_dataset_path = os.path.join(loading_path, 'TestingDataset.mat') \n",
    "    \n",
    "    # Load Training Dataset\n",
    "    with h5py.File(training_dataset_path, 'r') as f_training_dataset:\n",
    "        training_dataset = f_training_dataset['TrainingDataset']\n",
    "        subject_cell_ref = training_dataset[subject_id, 0]\n",
    "        subject_data = f_training_dataset[subject_cell_ref]\n",
    "        X_train_dataset = np.transpose(subject_data['x'][:], (0, 1, 2))\n",
    "        Y_train_dataset = np.squeeze(subject_data['y'][:])\n",
    "\n",
    "    # Load Testing Dataset\n",
    "    f_testing_dataset = scipy.io.loadmat(testing_dataset_path)\n",
    "    testing_dataset = f_testing_dataset['TestingDataset'][0, subject_id]\n",
    "    X_test_dataset = np.transpose(np.array(testing_dataset['x'][0][0]), (2, 1, 0))\n",
    "    #X_test_dataset = np.abs(X_test_dataset) #Added pre-aligment data is complex\n",
    "    Y_test_dataset = np.squeeze(np.array(testing_dataset['y'][0][0]))\n",
    "\n",
    "\n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    data = {\n",
    "        \"X_train_dataset\": X_train_dataset,\n",
    "        \"Y_train_dataset\": Y_train_dataset,\n",
    "        \"X_test_dataset\": X_test_dataset,\n",
    "        \"Y_test_dataset\": Y_test_dataset,\n",
    "    }\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bdef5b4-619c-4ee9-800b-173a3fff4bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recentering function \n",
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from pyriemann.utils.tangentspace import tangent_space\n",
    "\n",
    "def Recentering_TSProjection(C, M):\n",
    "    \"\"\"\n",
    "    1. Centers each covariance matrix: M^(-1/2) C M^(-1/2))\n",
    "    2. Projects to TS (& vectorises) using identity matrix as 'new centre'\n",
    "    \n",
    "    Parameters:\n",
    "        C (ndarray): set of covariance matrices (Nt, Nc, Nc) \n",
    "        M (ndarray): Reference matrix (Taken as log-eulid mean)\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: The log-transformed matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    Nt, _, _ = C.shape # Number of trials \n",
    "    \n",
    "    M_inv_sqrt = fractional_matrix_power(M, -0.5) # Compute M^(-1/2)\n",
    "    \n",
    "    # 1. Recentering step (on each covariance matrix) \n",
    "    X_centered = np.array([M_inv_sqrt @ C[i] @ M_inv_sqrt for i in range(Nt)])\n",
    "    #print(X_centered.shape)\n",
    "    \n",
    "    #2. TS projection \n",
    "    Centered_M = np.eye(C.shape[1]) # New center after centering is the identity matrix \n",
    "    X_TS = tangent_space(X_centered, Centered_M) # Project to TS (& vectorise)\n",
    "\n",
    "    \n",
    "    return X_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791e85b5-0a1d-4152-9aa1-66142cd51230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Rescaling(C):\n",
    "    \"\"\"\n",
    "    Rescale all tangent vectors: c_tilde = c / (1/N_t * sum_n ||c_n||)\n",
    "\n",
    "    Parameters:\n",
    "        C (ndarray): Collection of tangent vectors (c_n) to be rescaled\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Rescaled tangents vectors `c_tilde`.\n",
    "    \"\"\"\n",
    "    \n",
    "    Nt = C.shape[0] # Number of vectors\n",
    "    \n",
    "    # Compute the average magnitude (1/N_t * sum(||c_n||))\n",
    "    avg_magnitude = np.sum(np.linalg.norm(C, axis=1)) / Nt\n",
    "\n",
    "    # Normalize all vectors simultaneously\n",
    "    c_tilde  = C / avg_magnitude\n",
    "\n",
    "    return c_tilde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51400842-f616-4526-9fda-65c67a55bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acacf625-0acd-470a-9907-493381930aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "New_dataset = \"Yes\"\n",
    "\n",
    "if New_dataset == \"No\":\n",
    "    TransformedTraining_path = os.path.join(SavingPath, 'TransformedTraining.pkl')\n",
    "    TransformedTesting_path = os.path.join(SavingPath, 'TransformedTesting.pkl')\n",
    "\n",
    "    TransformedTraining = load_data(TransformedTraining_path)\n",
    "    TransformedTesting = load_data(TransformedTesting_path)\n",
    "\n",
    "elif New_dataset == \"Yes\":\n",
    "    TransformedTraining = {}\n",
    "    TransformedTesting = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "373ac8e3-8962-454a-b050-74a9b2848704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0:\n",
      "Loaded datatsets shape: \n",
      "(43450, 108, 100)\n",
      "(2777, 108, 100)\n",
      "Centred shapes: \n",
      "(43450, 5886)\n",
      "(2777, 5886)\n",
      "Rescaled shapes: \n",
      "(43450, 5886)\n",
      "(2777, 5886)\n",
      "Processing Class 0\n",
      "Nd explaining 95% variance:\n",
      "40\n",
      "Processing Class 1\n",
      "Nd explaining 95% variance:\n",
      "143\n",
      "Anchor points shapes: \n",
      "(5886, 6)\n",
      "(5886, 6)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 1)\n",
      "(1, 5886)\n",
      "(2777, 5886)\n",
      "Rotated Test: \n",
      "(2777, 5886)\n",
      "Subject 1:\n",
      "Loaded datatsets shape: \n",
      "(43419, 108, 100)\n",
      "(2808, 108, 100)\n",
      "Centred shapes: \n",
      "(43419, 5886)\n",
      "(2808, 5886)\n",
      "Rescaled shapes: \n",
      "(43419, 5886)\n",
      "(2808, 5886)\n",
      "Processing Class 0\n",
      "Nd explaining 95% variance:\n",
      "38\n",
      "Processing Class 1\n",
      "Nd explaining 95% variance:\n",
      "134\n",
      "Anchor points shapes: \n",
      "(5886, 6)\n",
      "(5886, 6)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(2808, 5886)\n",
      "Rotated Test: \n",
      "(2808, 5886)\n",
      "Subject 2:\n",
      "Loaded datatsets shape: \n",
      "(43732, 108, 100)\n",
      "(2495, 108, 100)\n",
      "Centred shapes: \n",
      "(43732, 5886)\n",
      "(2495, 5886)\n",
      "Rescaled shapes: \n",
      "(43732, 5886)\n",
      "(2495, 5886)\n",
      "Processing Class 0\n",
      "Nd explaining 95% variance:\n",
      "40\n",
      "Processing Class 1\n",
      "Nd explaining 95% variance:\n",
      "143\n",
      "Anchor points shapes: \n",
      "(5886, 6)\n",
      "(5886, 6)\n",
      "c_st shape: \n",
      "(5886, 5886)\n",
      "truncated U and VT: \n",
      "(5886, 2)\n",
      "(2, 5886)\n",
      "(2495, 5886)\n",
      "Rotated Test: \n",
      "(2495, 5886)\n"
     ]
    }
   ],
   "source": [
    "for sub_id in [0, 1, 2]:\n",
    "    print(f\"Subject {sub_id}:\")\n",
    "    subject_data = load_subject_data(LoadingPath_Non, sub_id, n_components=Nc)\n",
    "\n",
    "    X_train_dataset = subject_data[\"X_train_dataset\"]\n",
    "    Y_train_dataset = subject_data[\"Y_train_dataset\"]\n",
    "    X_test_dataset = subject_data[\"X_test_dataset\"]\n",
    "    Y_test_dataset = subject_data[\"Y_test_dataset\"]\n",
    "\n",
    "    # Sanity check\n",
    "    print(\"Loaded datatsets shape: \")\n",
    "    print(X_train_dataset.shape)\n",
    "    print(X_test_dataset.shape)\n",
    "    \n",
    "# ---------------------- 1. Recentre - recentres all trials to have centre of mass equal to identity matrix, followed by tangent space mapping (ref = I) ---------------------- \n",
    "    # Find covariance matrices\n",
    "    cov_estimator = Covariances(estimator='lwf')\n",
    "    Train_cov = cov_estimator.transform(X_train_dataset)\n",
    "    Test_cov = cov_estimator.transform(X_test_dataset)\n",
    "\n",
    "    # Find log-euclid centre ('M')\n",
    "    Train_M = mean_logeuclid(Train_cov)\n",
    "    Test_M = mean_logeuclid(Test_cov)\n",
    "\n",
    "    # Centre and project to TS (& vectorise)\n",
    "    Train_Centered = Recentering_TSProjection(Train_cov, Train_M)\n",
    "    Test_Centered = Recentering_TSProjection(Test_cov, Test_M)\n",
    "\n",
    "    #Sanity Check\n",
    "    print(\"Centred shapes: \")\n",
    "    print(Train_Centered.shape)\n",
    "    print(Test_Centered.shape)\n",
    "\n",
    "# ---------------------- 2. Rescale - match matrix dispersion around mean in both 'source' and 'target' (setting the average norm within set to be 1) ----------------------\n",
    "    Train_Rescale = Rescaling(Train_Centered)\n",
    "    Test_Rescale = Rescaling(Test_Centered)\n",
    "\n",
    "    #Sanity Check\n",
    "    print(\"Rescaled shapes: \")\n",
    "    print(Train_Rescale.shape)\n",
    "    print(Test_Rescale.shape)\n",
    "\n",
    "# ---------------------- 3. Rotation (Alignment of 'target' vectors) - Align each mean of each class as much as possible (using Eulidea Procrustes procedure)  ----------------------\n",
    "# ---------------------- Once target vectors are aligned, can be used with models trained using 'Train_Rescale' ---------------------- \n",
    "    \n",
    "    # Parameters\n",
    "    K = 2  # Number of classes\n",
    "    Ng = 3  # Number of clusters\n",
    "    #Nd = Train_Rescale.shape[1]  # Number of PCA components\n",
    "\n",
    "    # Dictionary to store the anchor points\n",
    "    source_anchor_points = []\n",
    "    target_anchor_points = []\n",
    "\n",
    "    for class_label in range(K):\n",
    "        print(f\"Processing Class {class_label}\")\n",
    "\n",
    "        # Extract class-specific data\n",
    "        s_class = Train_Rescale[Y_train_dataset == class_label]  # Source class-specific samples for class\n",
    "        t_class = Test_Rescale[Y_test_dataset == class_label]  # Target class-specific samples for class\n",
    "\n",
    "        # Train PCA on the source data (for this class)\n",
    "        pca = PCA()\n",
    "        pca.fit(s_class)\n",
    "        explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "        Nd_95 = np.argmax(explained_variance_ratio >= 0.95) + 1 # Find Nd that explains 95% variance\n",
    "        print(\"Nd explaining 95% variance:\")\n",
    "        print(Nd_95)\n",
    "        \n",
    "        pca = PCA(n_components=Nd_95) \n",
    "        s_pca = pca.fit_transform(s_class)\n",
    "        t_pca = pca.transform(t_class)  # Apply the trained PCA to the target\n",
    "\n",
    "        # Apply K-Means clustering (same for source and target)\n",
    "        kmeans_source = KMeans(n_clusters=Ng, random_state=42, n_init=10).fit(s_pca)\n",
    "        kmeans_target = KMeans(n_clusters=Ng, random_state=42, n_init=10).fit(t_pca)\n",
    "\n",
    "        # Compute the mean of each cluster (centroids)\n",
    "        source_means = kmeans_source.cluster_centers_  \n",
    "        target_means = kmeans_target.cluster_centers_  \n",
    "        \n",
    "        # Back-project cluster means to Nf dimensionality (i.e. original feature space)\n",
    "        source_means_Nf = (source_means @ pca.components_) + pca.mean_  # Shape: (Ng, Nf)\n",
    "        target_means_Nf = (target_means @ pca.components_) + pca.mean_  # Shape: (Ng, Nf)\n",
    "        \n",
    "        # Store anchor points \n",
    "        source_anchor_points.append(source_means_Nf)  \n",
    "        target_anchor_points.append(target_means_Nf)  \n",
    "\n",
    "    source_anchor_points = np.vstack(source_anchor_points)  \n",
    "    target_anchor_points = np.vstack(target_anchor_points)  \n",
    "\n",
    "    # Convert rows into columns (to make expected shape (Nf, Ng * K)\n",
    "    Train_AnchorPoints = source_anchor_points.T\n",
    "    Test_AnchorPoints = target_anchor_points.T\n",
    "\n",
    "    # Sanity Check \n",
    "    print(\"Anchor points shapes: \")\n",
    "    print(Train_AnchorPoints.shape)\n",
    "    print(Test_AnchorPoints.shape)\n",
    "\n",
    "    # Cross-product matrix \n",
    "    c_st = Train_AnchorPoints @ Test_AnchorPoints.T\n",
    "\n",
    "    #Sanity check - should be (Nf, Nf)\n",
    "    print(\"c_st shape: \")\n",
    "    print(c_st.shape) \n",
    "\n",
    "    # Perform Singular value decomposition on c_st\n",
    "    U, D, VT = np.linalg.svd(c_st, full_matrices=False)\n",
    "\n",
    "    # Find number of Nv vectors that explains 99.9% varaince \n",
    "    explained_variance = D**2\n",
    "    total_variance = np.sum(explained_variance)\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance) / total_variance\n",
    "    Nv = np.argmax(cumulative_explained_variance >= 0.999) + 1  # +1 because of 0-based indexing\n",
    "\n",
    "    U_tilde = U[:, :Nv] # Truncate using only Nv vectors\n",
    "    VT_tilde = VT[:Nv, :]\n",
    "\n",
    "    # Sanity check \n",
    "    print(\"truncated U and VT: \")\n",
    "    print(U_tilde.shape)\n",
    "    print(VT_tilde.shape)\n",
    "\n",
    "    print(Test_Rescale.shape)\n",
    "\n",
    "    Nt = Test_Rescale.shape[0]\n",
    "    Test_Rotated = np.zeros_like(Test_Rescale) # Initialise storage\n",
    "\n",
    "    # Align each testing trial\n",
    "    for t in range(Nt):\n",
    "        Test_Rotated[t] = U_tilde @ VT_tilde @ Test_Rescale[t]\n",
    "\n",
    "    print(\"Rotated Test: \")\n",
    "    print(Test_Rotated.shape)\n",
    "    \n",
    "    # Store per subject (transformed training and testing datasets)\n",
    "    TransformedTraining[sub_id] = Train_Rescale\n",
    "    TransformedTesting[sub_id] = Test_Rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73276bdc-1918-40cb-a1c1-188fedc89067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 19.154185036818188 minutes\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be99b3ce-8d43-4562-8f61-380f3e4533ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nicole/Documents/AudioCueWalking_analysis/Variables/AdaptVsNon/LOSO_CV/Dataset/Ns_100/Advance/Transformed/TangentSpace_PS/Transformed/Baseline/Clusters_Cst'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SavingPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcaa661-5e09-451b-9fb1-958eebfb1877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define saving paths for the variables\n",
    "training_saving_path = os.path.join(SavingPath, 'TransformedTraining_sub001_003.pkl')\n",
    "testing_saving_path = os.path.join(SavingPath, 'TransformedTesting_sub001_003.pkl')\n",
    "\n",
    "# Save TransformedTraining\n",
    "with open(training_saving_path, 'wb') as file:\n",
    "    pickle.dump(TransformedTraining, file)\n",
    "\n",
    "# Save TransformedTesting\n",
    "with open(testing_saving_path, 'wb') as file:\n",
    "    pickle.dump(TransformedTesting, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f5c18-f1f1-45a0-95b7-79900ed9b4f9",
   "metadata": {},
   "source": [
    "# Extra/Old scripts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d72e6d63-5e6e-4f2e-b050-fa8c6c7f371b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parameters\n",
    "K = 2  # Number of classes\n",
    "Ng = 3  # Number of clusters\n",
    "Nd = Train_Rescale.shape[1]  # Number of PCA components\n",
    "\n",
    "# Dictionary to store the anchor points\n",
    "source_anchor_points = []\n",
    "target_anchor_points = []\n",
    "\n",
    "for class_label in range(K):\n",
    "    print(f\"Processing Class {class_label}\")\n",
    "\n",
    "    # Extract class-specific data\n",
    "    s_class = Train_Rescale[Y_train_dataset == class_label]  # Source class-specific samples for class\n",
    "    t_class = Test_Rescale[Y_test_dataset == class_label]  # Target class-specific samples for class\n",
    "\n",
    "    # Train PCA on the source data (for this class)\n",
    "    pca = PCA(n_components=Nd) # Use full Nf dimensions\n",
    "    s_pca = pca.fit_transform(s_class)\n",
    "    t_pca = pca.transform(t_class)  # Apply the trained PCA to the target\n",
    "\n",
    "    # Apply K-Means clustering (same for source and target)\n",
    "    kmeans_source = KMeans(n_clusters=Ng, random_state=42, n_init=10).fit(s_pca)\n",
    "    kmeans_target = KMeans(n_clusters=Ng, random_state=42, n_init=10).fit(t_pca)\n",
    "\n",
    "    # Compute the mean of each cluster (centroids) \n",
    "    source_means = kmeans_source.cluster_centers_  \n",
    "    target_means = kmeans_target.cluster_centers_  \n",
    "\n",
    "    # Store anchor points \n",
    "    source_anchor_points.append(source_means)  \n",
    "    target_anchor_points.append(target_means)  \n",
    "    \n",
    "source_anchor_points = np.vstack(source_anchor_points)  \n",
    "target_anchor_points = np.vstack(target_anchor_points)  \n",
    "\n",
    "# Convert rows into columns (to make expected shape (Nf, Ng * K)\n",
    "Train_AnchorPoints = source_anchor_points.T\n",
    "Test_AnchorPoints = target_anchor_points.T\n",
    "\n",
    "# Sanity Check \n",
    "print(\"Anchor points shapes: \")\n",
    "print(Train_AnchorPoints.shape)\n",
    "print(Test_AnchorPoints.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9498d26-506b-4628-80cf-74b420370204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyRiemann",
   "language": "python",
   "name": "pyriemann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
